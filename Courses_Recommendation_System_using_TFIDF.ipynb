{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based Filtering by getting the similarity between User Course title tf-idf and all courses' technical and soft skills we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to learn user preferences, and locate or recommend items that are “similar” to the user preference.\n",
    "#### so we have information for each job title that helps us in measuring similarity between this job title's information and the information that each course have , and returns courses titles that are related really to what the user wants. \n",
    "\n",
    "#### Also using tf-idf features will help us a lot in getting the similarity of user job title's skill and the courses skills.\n",
    "#### In our case here the user preference would be his “Job title”, and the information we have for this job title is job's skill. And we are going to locate or recommend courses titles which their skills are similar to the User job title input's skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask,  jsonify,request\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading jobs_data csvfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1- import jobs_skills dataset \n",
    "'''\n",
    "\n",
    "original_data_jobs_skills = pd.read_csv('jobs_skills.csv') #original data loaded\n",
    "original_data_jobs_skills.columns = ['id', 'title', 'jobs_skills','industry','skills'] # cloumns we have\n",
    "\n",
    "enhanced_data_jobs_skills=pd.read_csv('jobs_skills.csv') #enhanced data will be saved here\n",
    "enhanced_data_jobs_skills.columns = ['id', 'title', 'jobs_skills','industry','skills']  # cloumns we have\n",
    "\n",
    "#enhanced_data_jobs_skills.sample(5)# discover data we have\n",
    "\n",
    "'''\n",
    "2- import courses dataset \n",
    "'''\n",
    "original_data_loaded = pd.read_csv('courses.csv') #original data loaded\n",
    "original_data_loaded.columns = ['id','_id', 'title', 'url','description', 'syllabus','skills','ratings_count','prerequisites','duration','category','level','schoolName','instructors','enrolled_students_count','avg_rating','num_reviews','price','source'] # cloumns we have\n",
    "\n",
    "enhanced_data=pd.read_csv('courses.csv') #enhanced data will be saved here\n",
    "enhanced_data.columns = ['id','_id', 'title', 'url','description', 'syllabus','skills','ratings_count','prerequisites','duration','category','level','schoolName','instructors','enrolled_students_count','avg_rating','num_reviews','price','source'] # cloumns we have\n",
    "\n",
    "#enhanced_data.sample(10)# discover data we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\AnacondaSetup\\envs\\New_Env\\lib\\site-packages\\ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1- Pre Processing jobs_skills dataset\n",
    "'''\n",
    "#removing special characters\n",
    "enhanced_data_jobs_skills['title']=enhanced_data_jobs_skills['title'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "enhanced_data_jobs_skills['skills']=enhanced_data_jobs_skills['skills'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "\n",
    "'''-----------------Get rows that have 'nan' values-------------------'''\n",
    "# detect all the rows that contain 'nan' value from the enhanced enhanced_df\n",
    "indexNames1 = enhanced_data_jobs_skills[ (enhanced_data_jobs_skills['title'] == \"nan\")  | (enhanced_data_jobs_skills['skills'] == \"['nan']\")].index\n",
    "\n",
    "'''-----------------Get rows that have 'nan' values-------------------'''\n",
    "# detect all the rows that contain 'nan' value from the enhanced enhanced_df\n",
    "indexNames2 = enhanced_data_jobs_skills[ (enhanced_data_jobs_skills['title'] == \"\") | (enhanced_data_jobs_skills['skills'] == \"[]\")].index\n",
    "\n",
    "'''-------------Delete Rows that contains 'nan' values----------------'''\n",
    "# delete all rows for which column 'title','jobFunction','industry' has value'nan' from the original_df\n",
    "original_data_jobs_skills.drop(indexNames1 , inplace=True)\n",
    "\n",
    "# delete all rows for which column 'title','jobFunction','industry' has value'nan' from the enhanced_df\n",
    "enhanced_data_jobs_skills.drop(indexNames1 , inplace=True)\n",
    "\n",
    "'''-------------Delete Rows that contains 'nan' values----------------'''\n",
    "# delete all rows for which column 'title','jobFunction','industry' has value'nan' from the original_df\n",
    "original_data_jobs_skills.drop(indexNames2 , inplace=True)\n",
    "\n",
    "# delete all rows for which column 'title','jobFunction','industry' has value'nan' from the enhanced_df\n",
    "enhanced_data_jobs_skills.drop(indexNames2 , inplace=True)\n",
    "\n",
    "'''-----------------Drop Missing Values in Data Fram------------------'''\n",
    "# drop all rows that has missing values from the data fram original_df\n",
    "original_data_jobs_skills=original_data_jobs_skills.dropna(axis=0, how='any',inplace=False)\n",
    "\n",
    "# drop all rows that has missing values from the data fram enhanced_df\n",
    "enhanced_data_jobs_skills=enhanced_data_jobs_skills.dropna(axis=0, how='any',inplace=False)\n",
    "\n",
    "'''------------------Reset Indexies in Data Fram----------------------'''\n",
    "#reset the index of the original_df\n",
    "original_data_jobs_skills=original_data_jobs_skills.reset_index(drop=True)\n",
    "\n",
    "#reset the index of the enhanced_df\n",
    "enhanced_data_jobs_skills=enhanced_data_jobs_skills.reset_index(drop=True)\n",
    "\n",
    "#reset column 'id' in original_df\n",
    "original_data_jobs_skills['id'] = enhanced_data_jobs_skills.index\n",
    "\n",
    "#reset column 'id' in the enhanced_df\n",
    "enhanced_data_jobs_skills['id'] = enhanced_data_jobs_skills.index\n",
    "\n",
    "'''-----------tokenize - remove stop words - lemmatization - Stemming----------------'''\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "# excluding 'it' from the stopwords as it has meaning in the jobs the file have like 'IT/Software'\n",
    "stop= set(stopwords.words('english')) - set(['it'])\n",
    "\n",
    "#tokenization will done here too\n",
    "#lower case all letters\n",
    "enhanced_data_jobs_skills['title'] = enhanced_data_jobs_skills['title'].str.lower().str.replace(r'[^\\w\\s]+', ' ').str.split()\n",
    "enhanced_data_jobs_skills['skills'] = enhanced_data_jobs_skills['skills'].str.lower().str.replace(r'[^\\w\\s]+', ' ').str.split()\n",
    "\n",
    "\n",
    "#removing stop words\n",
    "enhanced_data_jobs_skills['title'] = enhanced_data_jobs_skills['title'].apply(lambda x: [item for item in x if item not in stop])\n",
    "enhanced_data_jobs_skills['skills'] = enhanced_data_jobs_skills['skills'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "#lemmatization , return the base or dictionary from the words , which is know as lema\n",
    "enhanced_data_jobs_skills['title']= enhanced_data_jobs_skills['title'].apply(lambda x : [lemma.lemmatize(y) for y in x])\n",
    "enhanced_data_jobs_skills['skills']= enhanced_data_jobs_skills['skills'].apply(lambda x : [lemma.lemmatize(y) for y in x])\n",
    "\n",
    "#Stemming , return the roots of the words and replacing the suffix, which is know as stem\n",
    "enhanced_data_jobs_skills['title']= enhanced_data_jobs_skills['title'].apply(lambda x : [ps.stem(y) for y in x])\n",
    "enhanced_data_jobs_skills['skills']= enhanced_data_jobs_skills['skills'].apply(lambda x : [ps.stem(y) for y in x])\n",
    "\n",
    "'''concatinate again all the values in each row into one string '''\n",
    "# each row in 'title' column\n",
    "c=0\n",
    "for item in enhanced_data_jobs_skills['title']:\n",
    "    s=\"\"\n",
    "    for i in item:\n",
    "        if (i != item[-1]):\n",
    "            s=s+str(i)+\" \"\n",
    "        else:\n",
    "            s=s+str(i)\n",
    "            \n",
    "    enhanced_data_jobs_skills['title'][c]=s\n",
    "    c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\AnacondaSetup\\envs\\New_Env\\lib\\site-packages\\ipykernel_launcher.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "G:\\AnacondaSetup\\envs\\New_Env\\lib\\site-packages\\ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2- Pre Processing courses dataset\n",
    "'''\n",
    "\n",
    "#removing special characters\n",
    "enhanced_data['title']=enhanced_data['title'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "enhanced_data['description']=enhanced_data['description'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "enhanced_data['skills']=enhanced_data['skills'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "\n",
    "'''-----------------Get rows that have 'nan' values-------------------'''\n",
    "# detect all the rows that contain 'nan' value from the enhanced enhanced_df\n",
    "indexNames = enhanced_data[ (enhanced_data['title'] == \"nan\") | (enhanced_data['description'] == \"nan\") | (enhanced_data['skills'] == \"['nan']\")].index\n",
    "\n",
    "'''-----------------Get rows that have 'nan' values-------------------'''\n",
    "# detect all the rows that contain 'nan' value from the enhanced enhanced_df\n",
    "indexNames = enhanced_data[ (enhanced_data['title'] == \"\") | (enhanced_data['description'] == \"\") | (enhanced_data['skills'] == \"[]\")].index\n",
    "\n",
    "'''-------------Delete Rows that contains 'nan' values----------------'''\n",
    "# delete all rows for which column 'title','jobFunction','industry' has value'nan' from the original_df\n",
    "original_data_loaded.drop(indexNames , inplace=True)\n",
    "\n",
    "# delete all rows for which column 'title','jobFunction','industry' has value'nan' from the enhanced_df\n",
    "enhanced_data.drop(indexNames , inplace=True)\n",
    "\n",
    "'''-----------------Drop Missing Values in Data Fram------------------'''\n",
    "# drop all rows that has missing values from the data fram original_df\n",
    "original_data_loaded=original_data_loaded.dropna(axis=0, how='any',inplace=False)\n",
    "\n",
    "# drop all rows that has missing values from the data fram enhanced_df\n",
    "enhanced_data=enhanced_data.dropna(axis=0, how='any',inplace=False)\n",
    "\n",
    "'''------------------Reset Indexies in Data Fram----------------------'''\n",
    "#reset the index of the original_df\n",
    "original_data_loaded=original_data_loaded.reset_index(drop=True)\n",
    "\n",
    "#reset the index of the enhanced_df\n",
    "enhanced_data=enhanced_data.reset_index(drop=True)\n",
    "\n",
    "#reset column 'id' in original_df\n",
    "original_data_loaded['id'] = enhanced_data.index\n",
    "\n",
    "#reset column 'id' in the enhanced_df\n",
    "enhanced_data['id'] = enhanced_data.index\n",
    "\n",
    "'''-----------tokenize - remove stop words - lemmatization - Stemming----------------'''\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "# excluding 'it' from the stopwords as it has meaning in the jobs the file have like 'IT/Software'\n",
    "stop= set(stopwords.words('english')) - set(['it'])\n",
    "\n",
    "#tokenization will done here too\n",
    "#lower case all letters\n",
    "enhanced_data['title'] = enhanced_data['title'].str.lower().str.replace(r'[^\\w\\s]+', ' ').str.split()\n",
    "enhanced_data['skills'] = enhanced_data['skills'].str.lower().str.replace(r'[^\\w\\s]+', ' ').str.split()\n",
    "\n",
    "#removing stop words\n",
    "enhanced_data['title'] = enhanced_data['title'].apply(lambda x: [item for item in x if item not in stop])\n",
    "enhanced_data['skills'] = enhanced_data['skills'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "#lemmatization , return the base or dictionary from the words , which is know as lema\n",
    "enhanced_data['title']= enhanced_data['title'].apply(lambda x : [lemma.lemmatize(y) for y in x])\n",
    "enhanced_data['skills']= enhanced_data['skills'].apply(lambda x : [lemma.lemmatize(y) for y in x])\n",
    "\n",
    "#Stemming , return the roots of the words and replacing the suffix, which is know as stem\n",
    "enhanced_data['title']= enhanced_data['title'].apply(lambda x : [ps.stem(y) for y in x])\n",
    "enhanced_data['skills']= enhanced_data['skills'].apply(lambda x : [ps.stem(y) for y in x])\n",
    "\n",
    "\n",
    "'''concatinate again all the values in each row into one string '''\n",
    "# each row in 'title' column\n",
    "c=0\n",
    "for item in enhanced_data['title']:\n",
    "    s=\"\"\n",
    "    for i in item:\n",
    "        if (i != item[-1]):\n",
    "            s=s+str(i)+\" \"\n",
    "        else:\n",
    "            s=s+str(i)\n",
    "            \n",
    "    enhanced_data['title'][c]=s\n",
    "    c=c+1\n",
    "    \n",
    "\n",
    "    \n",
    "# each row in 'skills' column\n",
    "c=0\n",
    "for item in enhanced_data['skills']:\n",
    "    s=\"\"\n",
    "    for i in item:\n",
    "        if (i != item[-1]):\n",
    "            s=s+str(i)+\" \"\n",
    "        else:\n",
    "            s=s+str(i)\n",
    "            \n",
    "    enhanced_data['skills'][c]=s\n",
    "    c=c+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Enhanced data in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the enhanced_data_jobs_skills in jobs_and_skills.csv file,so we don't have to reclean the data each time we want to use it again \n",
    "#enhanced_data_jobs_skills.to_csv('jobs_and_skills.csv')\n",
    "\n",
    "# save the enhanced_data in enhanced_courses.csv file,so we don't have to reclean the data each time we want to use it again \n",
    "#enhanced_data.to_csv('enhanced_courses.csv')\n",
    "\n",
    "#upload jobs_and_skills.csv\n",
    "#enhanced_data_jobs_skills = pd.read_csv('jobs_and_skills.csv')\n",
    "\n",
    "#upload enhanced_courses.csv\n",
    "#enhanced_data = pd.read_csv('enhanced_courses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have our Data Loaded Tokenized, Stemmed, Lemmatizied, Lowercased, no Special Characters no Missing Values, no 'nan' Values\n",
    "\n",
    "\n",
    "# Now we are ready to use our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get skill from original data frame\n",
    "'''\n",
    "def get_skill_from_original_original_df(id):  \n",
    "    return original_data_loaded.loc[original_data_loaded['id'] == id]['skills'].tolist()[0] # Just reads the results out of the dictionary.\n",
    "\n",
    "'''\n",
    "get skill from enhanced data frame\n",
    "'''\n",
    "def get_skill_from_enhanced_df(id):  \n",
    "    return enhanced_data.loc[enhanced_data['id'] == id]['skills'].tolist()[0] # Just reads the results out of the dictionary.\n",
    "\n",
    "'''\n",
    "get title from original data frame\n",
    "'''\n",
    "def get_title_from_original_original_df(id):  \n",
    "    return original_data_loaded.loc[original_data_loaded['id'] == id]['title'].tolist()[0] # Just reads the results out of the dictionary.\n",
    "\n",
    "'''\n",
    "convert string into list\n",
    "'''\n",
    "def Convert(string): \n",
    "    li = list(string.split(\" \")) \n",
    "    return li "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>jobs_skills</th>\n",
       "      <th>industry</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10055</th>\n",
       "      <td>10055</td>\n",
       "      <td>Office Admin</td>\n",
       "      <td>['Administration']</td>\n",
       "      <td>['Information Technology Services', 'Internet/...</td>\n",
       "      <td>['Office Management', 'Administration', 'Compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70438</th>\n",
       "      <td>70438</td>\n",
       "      <td>Project Leader (ERP Implementation)</td>\n",
       "      <td>['IT/Software Development', 'Engineering - Tel...</td>\n",
       "      <td>['Information Technology Services', 'Computer ...</td>\n",
       "      <td>['Software Development', 'Computer Science', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40068</th>\n",
       "      <td>40068</td>\n",
       "      <td>Sales Account Manager - Bahrain</td>\n",
       "      <td>['Marketing/PR/Advertising', 'Sales/Retail', '...</td>\n",
       "      <td>['Information Technology Services', 'Computer ...</td>\n",
       "      <td>['Customer Support', 'Startup', 'Amazon', 'Sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691</th>\n",
       "      <td>6691</td>\n",
       "      <td>IT Help Desk Specialist</td>\n",
       "      <td>['IT/Software Development', 'Engineering - Tel...</td>\n",
       "      <td>['Real Estate/Property Management']</td>\n",
       "      <td>['Information Technology (IT)', 'Computer Scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95198</th>\n",
       "      <td>95198</td>\n",
       "      <td>Junior Sales Representative</td>\n",
       "      <td>['IT/Software Development', 'Sales/Retail']</td>\n",
       "      <td>['Information Technology Services', 'Computer ...</td>\n",
       "      <td>['Account Management', 'Sales', 'Sales Skills'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36951</th>\n",
       "      <td>36951</td>\n",
       "      <td>Quality Manager</td>\n",
       "      <td>['Medical/Healthcare', 'Pharmaceutical', 'Qual...</td>\n",
       "      <td>['Healthcare and Medical Services']</td>\n",
       "      <td>['Medical', 'Quality Management', 'Quality Ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29937</th>\n",
       "      <td>29937</td>\n",
       "      <td>Sales Representative - Real Estate</td>\n",
       "      <td>['Customer Service/Support', 'Sales/Retail']</td>\n",
       "      <td>['Real Estate/Property Management']</td>\n",
       "      <td>['Customer Support', 'Sales', 'Property', 'Rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96575</th>\n",
       "      <td>96575</td>\n",
       "      <td>Full-Stack Angular Developer</td>\n",
       "      <td>['IT/Software Development', 'Engineering - Tel...</td>\n",
       "      <td>['Healthcare and Medical Services', 'Informati...</td>\n",
       "      <td>['Information Technology (IT)', 'Web Developme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89223</th>\n",
       "      <td>89223</td>\n",
       "      <td>Billing Specialist</td>\n",
       "      <td>['Accounting/Finance', 'Administration']</td>\n",
       "      <td>['Information Technology Services']</td>\n",
       "      <td>['Accounting', 'Commerce', 'Billing', 'Finance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16705</th>\n",
       "      <td>16705</td>\n",
       "      <td>Mechanical Technical Office Engineer</td>\n",
       "      <td>['Engineering - Mechanical/Electrical']</td>\n",
       "      <td>['Real Estate/Property Management']</td>\n",
       "      <td>['Firefighting', 'HVAC', 'Technical Office', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                 title  \\\n",
       "10055  10055                          Office Admin   \n",
       "70438  70438   Project Leader (ERP Implementation)   \n",
       "40068  40068       Sales Account Manager - Bahrain   \n",
       "6691    6691               IT Help Desk Specialist   \n",
       "95198  95198           Junior Sales Representative   \n",
       "36951  36951                       Quality Manager   \n",
       "29937  29937    Sales Representative - Real Estate   \n",
       "96575  96575          Full-Stack Angular Developer   \n",
       "89223  89223                    Billing Specialist   \n",
       "16705  16705  Mechanical Technical Office Engineer   \n",
       "\n",
       "                                             jobs_skills  \\\n",
       "10055                                 ['Administration']   \n",
       "70438  ['IT/Software Development', 'Engineering - Tel...   \n",
       "40068  ['Marketing/PR/Advertising', 'Sales/Retail', '...   \n",
       "6691   ['IT/Software Development', 'Engineering - Tel...   \n",
       "95198        ['IT/Software Development', 'Sales/Retail']   \n",
       "36951  ['Medical/Healthcare', 'Pharmaceutical', 'Qual...   \n",
       "29937       ['Customer Service/Support', 'Sales/Retail']   \n",
       "96575  ['IT/Software Development', 'Engineering - Tel...   \n",
       "89223           ['Accounting/Finance', 'Administration']   \n",
       "16705            ['Engineering - Mechanical/Electrical']   \n",
       "\n",
       "                                                industry  \\\n",
       "10055  ['Information Technology Services', 'Internet/...   \n",
       "70438  ['Information Technology Services', 'Computer ...   \n",
       "40068  ['Information Technology Services', 'Computer ...   \n",
       "6691                 ['Real Estate/Property Management']   \n",
       "95198  ['Information Technology Services', 'Computer ...   \n",
       "36951                ['Healthcare and Medical Services']   \n",
       "29937                ['Real Estate/Property Management']   \n",
       "96575  ['Healthcare and Medical Services', 'Informati...   \n",
       "89223                ['Information Technology Services']   \n",
       "16705                ['Real Estate/Property Management']   \n",
       "\n",
       "                                                  skills  \n",
       "10055  ['Office Management', 'Administration', 'Compu...  \n",
       "70438  ['Software Development', 'Computer Science', '...  \n",
       "40068  ['Customer Support', 'Startup', 'Amazon', 'Sal...  \n",
       "6691   ['Information Technology (IT)', 'Computer Scie...  \n",
       "95198  ['Account Management', 'Sales', 'Sales Skills'...  \n",
       "36951  ['Medical', 'Quality Management', 'Quality Ass...  \n",
       "29937  ['Customer Support', 'Sales', 'Property', 'Rea...  \n",
       "96575  ['Information Technology (IT)', 'Web Developme...  \n",
       "89223  ['Accounting', 'Commerce', 'Billing', 'Finance...  \n",
       "16705  ['Firefighting', 'HVAC', 'Technical Office', '...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data_jobs_skills.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Recommend...\n",
    "#### Try to Enter this title : Senior Android Developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you have to enter a job title that already exists in the system! \n",
      "Enter your Job Title : Senior Android Developer\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1- Ask user to enter a course title have already existed in the system! \n",
    "'''\n",
    "print(\"you have to enter a job title that already exists in the system! \")\n",
    "\n",
    "#try this title\n",
    "#title = Senior Android Developer\n",
    "org = input(\"Enter your Job Title : \") \n",
    "\n",
    "user_input=\"\"\n",
    "user_input=org\n",
    "\n",
    "#removing special characters\n",
    "user_input=user_input.encode('ascii', 'ignore').decode('ascii')\n",
    "#lower case all letters\n",
    "user_input = user_input.lower()\n",
    "user_input = re.sub(r'[^\\w\\s]+',r' ', user_input)\n",
    "#Remove additional white spaces\n",
    "user_input = re.sub('[\\s]+', ' ', user_input)\n",
    "user_input = user_input.split()\n",
    "#removing stop words\n",
    "stop_words = set(stop) \n",
    "#word_tokens = word_tokenize(user_title) \n",
    "user_input = [w for w in user_input if not w in stop_words] \n",
    "#lemmatization , return the base or dictionary from the words , which is know as lema\n",
    "user_input = [lemma.lemmatize(w) for w in user_input] \n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# each row in 'skills' column\n",
    "s=\"\"\n",
    "for item in user_input:\n",
    "    if (item != user_input[-1]):\n",
    "        s=s+str(item)+\" \"\n",
    "    else:\n",
    "        s=s+str(item)\n",
    "    \n",
    "user_input=\"\"\n",
    "user_input=s\n",
    "\n",
    "validation_list=enhanced_data_jobs_skills[original_data_jobs_skills['title'] == org].index\n",
    "\n",
    "# settings that you use for count vectorizer will go here\n",
    "tf = TfidfVectorizer(use_idf=True)\n",
    "all_skills=list()\n",
    "if len(validation_list) == 0: \n",
    "    print(\"no such skills found in our system \")\n",
    "else:\n",
    "    index_of_row = original_data_jobs_skills[(original_data_jobs_skills['title'] == org)].index\n",
    "    #print(index_of_row)\n",
    "    \n",
    "    for item_id in index_of_row:\n",
    "        all_skills.append(enhanced_data_jobs_skills['skills'][item_id])\n",
    "        \n",
    "unique_skills=list()\n",
    "for each_list in all_skills:\n",
    "    for each_item in each_list:\n",
    "        if each_item not in unique_skills :\n",
    "            unique_skills.append(each_item)\n",
    "            \n",
    "# each row in 'skills' column\n",
    "c=0\n",
    "s=\"\"\n",
    "for item in unique_skills:\n",
    "    if (item != unique_skills[-1]):\n",
    "        s=s+str(item)+\" \"\n",
    "    else:\n",
    "        s=s+str(item)      \n",
    "    c=c+1\n",
    "    \n",
    "\n",
    "'''\n",
    "2- We will treat the user input as a document , as add it in the collection of skill documents we have.\n",
    "'''\n",
    "all_docs=[]\n",
    "for i in enhanced_data['skills']:\n",
    "    all_docs.append(i)\n",
    "\n",
    "all_docs.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\AnacondaSetup\\envs\\New_Env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "G:\\AnacondaSetup\\envs\\New_Env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "G:\\AnacondaSetup\\envs\\New_Env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "G:\\AnacondaSetup\\envs\\New_Env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "G:\\AnacondaSetup\\envs\\New_Env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "G:\\AnacondaSetup\\envs\\New_Env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#Function so that one session can be called multiple times. \n",
    "#Useful while multiple calls need to be done for embedding. \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_useT(module):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.placeholder(tf.string)\n",
    "        embed = hub.Module(module)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.train.MonitoredSession()\n",
    "    return lambda x: session.run(embeddings, {sentences: x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\AnacondaSetup\\envs\\New_Env\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\AnacondaSetup\\envs\\New_Env\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "embed_fn = embed_useT('G:\\sentence_wise_email\\module\\module_useT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed_fn(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_matrix = embed_fn(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "3- Get Tf-idf for all unique words we have in all documents\n",
    "each skill row will be trated as a document and each document contains some words.\n",
    "\n",
    "4- Measure similarity between user course title and the skills of all the courses  \n",
    "and return the most similar recommended title courses.\n",
    "'''\n",
    "\n",
    "# settings that you use for count vectorizer will go here\n",
    "tf = TfidfVectorizer(use_idf=True)\n",
    "results=list()\n",
    "\n",
    "\n",
    "tfidf_matrix = tf.fit_transform(all_docs)\n",
    "\n",
    "cosine_similarities = linear_kernel(tfidf_matrix[-1], tfidf_matrix) \n",
    "\n",
    "related_docs_indices = cosine_similarities[0][:-2].argsort()[:-101:-1]\n",
    "\n",
    "cosine_similarities[-1][related_docs_indices]\n",
    "\n",
    "'''\n",
    "5- Start Recommendation to the top 100 Course which their technical\n",
    "and soft skills are similar to the Title that the user entered\n",
    "'''  \n",
    "\n",
    "i=0\n",
    "list_result=list()\n",
    "recommended_courses_list=list()\n",
    "print(\"==================== Recommended 100 course titles ===================================\")\n",
    "\n",
    "for rec in cosine_similarities[0][related_docs_indices]: \n",
    "    #print(i+1)\n",
    "    print(\"Recommended course \", i+1 ,\": \",  get_title_from_original_original_df(related_docs_indices[i]))\n",
    "    print(\"(score: \" + str(rec) + \")\")\n",
    "\n",
    "    recommended_courses_list.append(get_title_from_original_original_df(related_docs_indices[i]))\n",
    "    print(\"\")\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
